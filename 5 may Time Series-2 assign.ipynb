{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c35a9-9954-4895-b7ab-f76b87dacd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is meant by time-dependent seasonal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b262d25-308c-43a7-a79f-c40f5b72f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Time-dependent seasonal components, in the context of time series analysis, refer to recurring patterns or variations in data that occur at specific time intervals or seasons and are influenced by external factors such as time of the year, month, week, or other cyclical patterns. These components represent systematic fluctuations in the data that repeat over time and can often be attributed to external factors, making them a fundamental part of time series decomposition.\n",
    "\n",
    "Key characteristics of time-dependent seasonal components include:\n",
    "\n",
    "1. **Regular Repetition:** Time-dependent seasonal components exhibit regular and predictable patterns that repeat at fixed intervals. For example, sales data might exhibit seasonal patterns with spikes in sales during the holiday season every year.\n",
    "\n",
    "2. **Influence of Calendar Time:** These components are influenced by calendar time, such as days of the week, months, seasons, or years. For example, retail sales may exhibit weekly or yearly seasonality.\n",
    "\n",
    "3. **External Factors:** Time-dependent seasonality is typically driven by external factors or events, such as weather conditions, holidays, cultural events, or business cycles. These factors can influence the underlying patterns in the data.\n",
    "\n",
    "4. **Additive or Multiplicative:** Seasonal components can be either additive or multiplicative. In additive seasonality, the seasonal effect is added to the underlying trend and error term, while in multiplicative seasonality, the seasonal effect is multiplied by the trend and error term.\n",
    "\n",
    "5. **Magnitude and Shape:** Seasonal components can vary in magnitude and shape over different time periods. For example, the sales increase during the holiday season may have a different magnitude and shape compared to other seasons.\n",
    "\n",
    "Identifying and modeling time-dependent seasonal components is crucial in time series forecasting and analysis because they can have a significant impact on the overall behavior of the time series data. Properly modeling and accounting for seasonal patterns allows for more accurate forecasts and a better understanding of underlying trends.\n",
    "\n",
    "Common techniques for handling time-dependent seasonal components include seasonal decomposition of time series data using methods like Seasonal Decomposition of Time Series (STL), Seasonal Decomposition of Time Series by LOESS (STL-LOESS), or seasonal decomposition using moving averages (SMA). These methods help separate the time series into its trend, seasonal, and residual components, making it easier to analyze and model each component separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7eb4e-95ff-45e2-a659-7759b7e45b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How can time-dependent seasonal components be identified in time series data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7690663-3662-4a62-bda3-6c0e50d22ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "Identifying time-dependent seasonal components in time series data involves recognizing recurring patterns or variations that occur at specific time intervals or seasons. Here are common methods and techniques for identifying time-dependent seasonal components in time series data:\n",
    "\n",
    "1. **Visual Inspection:**\n",
    "   - **Seasonal Plots:** Create seasonal subseries plots, also known as seasonal box plots or seasonal subseries graphs. These plots display data for each season separately, making it easier to spot recurring patterns visually.\n",
    "   - **Time Series Plots:** Plot the time series data over time to identify repeating patterns or cycles. Look for regular up-and-down movements that occur at fixed intervals.\n",
    "\n",
    "2. **Autocorrelation Function (ACF):**\n",
    "   - Calculate the autocorrelation function of the time series data. Seasonal patterns often result in spikes or peaks in the ACF at lags that correspond to the season's length. For example, for monthly data, you might expect to see spikes at lags 12, 24, 36, etc.\n",
    "\n",
    "3. **Seasonal Decomposition:**\n",
    "   - Apply seasonal decomposition techniques to break down the time series into its individual components, including the trend, seasonal, and residual components. Methods like Seasonal Decomposition of Time Series (STL) or Seasonal Decomposition of Time Series by LOESS (STL-LOESS) can help isolate the seasonal component.\n",
    "\n",
    "4. **Boxplots and Heatmaps:**\n",
    "   - Create boxplots or heatmaps to visualize the data across different seasons or time periods. This can help reveal patterns and variations in data across seasons.\n",
    "\n",
    "5. **Frequency Domain Analysis:**\n",
    "   - Use frequency domain analysis techniques, such as Fourier transforms or periodograms, to identify dominant frequency components in the data. Peaks in the frequency domain at specific frequencies may correspond to seasonal patterns.\n",
    "\n",
    "6. **Seasonal Decomposition with Moving Averages (SMA):**\n",
    "   - Apply seasonal decomposition using moving averages (SMA) to isolate the seasonal component from the original time series. SMA involves calculating centered moving averages or weighted moving averages to emphasize seasonal patterns.\n",
    "\n",
    "7. **Expert Knowledge:**\n",
    "   - Consult domain experts who have a deep understanding of the data and the factors that influence it seasonally. Expert knowledge can provide valuable insights into the presence of seasonal components and their characteristics.\n",
    "\n",
    "8. **Statistical Tests:**\n",
    "   - Use statistical tests to assess the presence of seasonality. For example, perform statistical tests like the Augmented Dickey-Fuller (ADF) test or the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test to check for stationarity and seasonality.\n",
    "\n",
    "9. **Machine Learning Models:**\n",
    "   - Train machine learning models, such as seasonal decomposition of time series models or autoregressive models with seasonal components (SARIMA), to capture and model seasonality in the data.\n",
    "\n",
    "10. **Exploratory Data Analysis (EDA):**\n",
    "    - Conduct exploratory data analysis, including aggregating and summarizing data at different seasonal frequencies (e.g., daily, weekly, monthly) to identify patterns.\n",
    "\n",
    "Remember that the identification of seasonal components may vary depending on the nature of the data and the specific problem you are addressing. It often involves a combination of visual inspection, statistical analysis, and domain knowledge to recognize recurring patterns and seasonality in time series data. Once identified, seasonal components can be used for forecasting and modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b906628-c68c-4768-a9d8-114f2b6363bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What are the factors that can influence time-dependent seasonal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25716dd-1ead-43db-8b5b-cb2e8d50906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Time-dependent seasonal components in time series data are influenced by various external factors and underlying phenomena. These factors can introduce recurring patterns or variations that repeat at specific time intervals or seasons. Here are some of the key factors that can influence time-dependent seasonal components:\n",
    "\n",
    "1. **Calendar Time:**\n",
    "   - The most common factor influencing seasonality is the calendar time itself. Events and patterns that occur regularly on a calendar, such as days of the week, months, seasons, and years, can lead to seasonal variations. For example, holidays, weekends, or annual cycles like school years or fiscal years can create seasonality in data.\n",
    "\n",
    "2. **Weather and Climate:**\n",
    "   - Weather conditions and climate patterns can have a significant impact on seasonality. Data related to temperature, precipitation, humidity, and other weather-related variables often exhibit seasonal patterns due to changes in seasons, weather seasons, or annual weather cycles.\n",
    "\n",
    "3. **Natural Phenomena:**\n",
    "   - Natural phenomena, such as biological cycles, can influence seasonality. For example, the mating or migration patterns of animals, plant growth cycles, or the breeding season for certain species can introduce seasonal components into ecological or biological data.\n",
    "\n",
    "4. **Economic Factors:**\n",
    "   - Economic factors, including consumer behavior and business cycles, can lead to seasonality in economic and financial time series data. Seasonal sales patterns, stock market fluctuations, and quarterly or annual reporting cycles can create seasonal variations.\n",
    "\n",
    "5. **Cultural and Social Events:**\n",
    "   - Cultural and social events, such as festivals, holidays, elections, and cultural practices, can drive seasonality. These events often result in changes in behavior, consumption, and activity levels.\n",
    "\n",
    "6. **Business and Industry-Specific Factors:**\n",
    "   - Different industries and businesses may experience seasonality based on their specific operations and customer behavior. For instance, the retail industry sees increased sales during holiday shopping seasons, while the tourism industry experiences peak seasons during specific months.\n",
    "\n",
    "7. **Environmental Factors:**\n",
    "   - Environmental factors, such as vegetation growth, water levels, or animal migrations, can introduce seasonality into ecological and environmental time series data.\n",
    "\n",
    "8. **Technological and Innovation Cycles:**\n",
    "   - Technological advancements and innovation cycles can lead to seasonality in data related to product releases, technology adoption, and trends in various industries.\n",
    "\n",
    "9. **Human Behavior and Habits:**\n",
    "   - Human behavior and habits, including changes in work schedules, commuting patterns, and leisure activities, can result in seasonality in data related to transportation, energy consumption, and leisure industries.\n",
    "\n",
    "10. **Geographic Location:**\n",
    "    - Geographic location can influence seasonality, especially in regions with distinct climates or geographical characteristics. Coastal regions may have different seasonal patterns from inland areas.\n",
    "\n",
    "11. **Regulatory and Policy Changes:**\n",
    "    - Regulatory changes or policies that are implemented or revised on a regular schedule, such as tax deadlines or government budget cycles, can lead to seasonality in data related to government and public finance.\n",
    "\n",
    "12. **Random Events:**\n",
    "    - In some cases, random events or noise in the data can create apparent seasonality. It is important to distinguish between genuine seasonal patterns and random fluctuations.\n",
    "\n",
    "Understanding the specific factors that influence time-dependent seasonal components is essential for accurate modeling and forecasting. Analyzing and modeling these seasonal variations can help businesses, researchers, and policymakers make informed decisions and predictions based on historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ee07b-108d-4447-b317-99eeb8bf4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How are autoregression models used in time series analysis and forecasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0284d-1cf3-499f-8a94-ec8081f05992",
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoregression models, often referred to as autoregressive (AR) models, are a class of time series models used in time series analysis and forecasting. These models are valuable for understanding and capturing the temporal dependencies and patterns present in time series data. Autoregressive models are based on the idea that the value of a variable at a given time point is linearly related to its previous values, creating a time-dependent relationship within the data.\n",
    "\n",
    "Here's how autoregressive models are used in time series analysis and forecasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad951f6-aa02-408a-9ab0-7ae152a42a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Modeling Time Dependencies:\n",
    "\n",
    "Autoregressive models capture the temporal dependencies within a time series. They express the value of the time series at a given time as a linear combination of its previous values. The order of the autoregressive model, denoted as \"p,\" indicates the number of lagged (previous) values considered in the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85cf96-69f2-4f02-9624-e7a9438caa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Formulation:\n",
    "\n",
    "An autoregressive model of order \"p,\" denoted as AR(p), is expressed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d349d-8d10-4f21-b467-050310fdf889",
   "metadata": {},
   "outputs": [],
   "source": [
    "X(t) = c + φ(1) * X(t-1) + φ(2) * X(t-2) + ... + φ(p) * X(t-p) + ε(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261debb-d9f0-4025-ac2e-3c516dfee371",
   "metadata": {},
   "outputs": [],
   "source": [
    "X(t) is the value of the time series at time t.\n",
    "φ(1), φ(2), ..., φ(p) are the autoregressive coefficients.\n",
    "c is a constant.\n",
    "ε(t) is the error term representing random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41fd307-114c-4803-84a4-e05211243003",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameter Estimation:\n",
    "\n",
    "The autoregressive coefficients (φ's) are estimated from historical time series data using methods like the method of moments, maximum likelihood estimation, or least squares.\n",
    "Model Selection:\n",
    "\n",
    "The order of the autoregressive model (p) is determined through model selection techniques, such as examining autocorrelation plots (ACF) and partial autocorrelation plots (PACF) or using information criteria like the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).\n",
    "Forecasting:\n",
    "\n",
    "Once the autoregressive model is trained and the coefficients are estimated, it can be used for forecasting future values of the time series. Forecasts are typically generated recursively by using the model's own predictions as inputs for future time points.\n",
    "Model Diagnostics:\n",
    "\n",
    "Autoregressive models undergo diagnostics to assess their goodness of fit. This includes checking for model assumptions like the stationarity of the time series and the independence and homoscedasticity of residuals.\n",
    "Prediction Intervals:\n",
    "\n",
    "Autoregressive models can be used to calculate prediction intervals, which provide a range of possible values within which future observations are likely to fall. Prediction intervals account for uncertainty in forecasting.\n",
    "Seasonal and Trend Components:\n",
    "\n",
    "Autoregressive models can be extended to capture additional components, such as seasonality and trend, by incorporating differencing or seasonal terms (e.g., SARIMA models) or combining autoregression with other models like exponential smoothing.\n",
    "Autoregressive models are particularly useful when time series data exhibit temporal dependencies, and their simplicity and interpretability make them a valuable tool in time series forecasting. However, it's important to note that autoregressive models assume that the time series is stationary, meaning its statistical properties remain constant over time. For non-stationary data, differencing or more complex models may be necessary.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a789d5-3f62-4747-a2c1-3ff2a93d6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How do you use autoregression models to make predictions for future time points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcbc2b6-fe59-4900-9b6b-ae7a6213ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoregression (AR) models are used to make predictions for future time points in a time series by leveraging the relationship between the current value and its past values. To use an AR model for forecasting, follow these steps:\n",
    "\n",
    "1. **Model Selection:** Determine the order of the autoregressive model (often denoted as \"p\"). This order specifies the number of lagged (previous) values of the time series to include in the model. Model selection can be done using techniques like autocorrelation plots (ACF) and partial autocorrelation plots (PACF) or by using information criteria like the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).\n",
    "\n",
    "2. **Model Estimation:** Estimate the autoregressive coefficients (φ's) for the selected order \"p\" using historical time series data. The coefficients are estimated through methods such as the method of moments, maximum likelihood estimation, or least squares.\n",
    "\n",
    "3. **Initial Values:** To make predictions, you'll need initial values for the lagged terms (X(t-1), X(t-2), ..., X(t-p)). These initial values can be obtained from the historical data.\n",
    "\n",
    "4. **Recursive Forecasting:** Once you have the autoregressive coefficients and initial values, you can start making predictions for future time points using the AR model. The general procedure is as follows:\n",
    "\n",
    "   - Begin with the initial values for the lagged terms (e.g., X(t-1), X(t-2), ..., X(t-p)) based on historical data.\n",
    "   - Use the estimated coefficients to calculate the forecasted value for the next time point, X(t+1), as a linear combination of the lagged terms and a constant term (if present in the model).\n",
    "   - Update the lagged terms by shifting them forward in time, replacing X(t-1) with X(t), X(t-2) with X(t-1), and so on.\n",
    "   - Repeat the process to forecast additional future time points.\n",
    "\n",
    "The recursive forecasting process is applied iteratively, with each forecasted value becoming part of the lagged terms for the next prediction. The number of forecasts you make into the future depends on your specific forecasting horizon.\n",
    "\n",
    "5. **Prediction Intervals:** When making predictions with an AR model, it's valuable to calculate prediction intervals. Prediction intervals provide a range within which future observations are likely to fall, taking into account the uncertainty in the forecasts. You can estimate prediction intervals using the model's residuals and statistical properties.\n",
    "\n",
    "6. **Model Diagnostics:** After forecasting, perform model diagnostics to assess the goodness of fit. Check for model assumptions such as stationarity, independence, and homoscedasticity of residuals. Residual analysis helps evaluate the model's adequacy and identify potential issues.\n",
    "\n",
    "7. **Monitoring and Updating:** Continuously monitor the performance of the AR model as new data becomes available. You may need to update the model's coefficients periodically to adapt to changing patterns or trends in the time series.\n",
    "\n",
    "It's important to note that AR models assume that the time series is stationary, meaning its statistical properties remain constant over time. For non-stationary data, preprocessing steps like differencing may be required before fitting an AR model. Additionally, for time series with seasonality or trend components, more advanced models like seasonal autoregressive integrated moving average (SARIMA) models may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230965e-8019-45df-9720-2f90dda09de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What is a moving average (MA) model and how does it differ from other time series models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a7e27-c640-480b-99cb-9349f4f6c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Moving Average (MA) model is a time series model used to represent and forecast a time series by taking into account the past observations' weighted average values. It is one of the components of more complex time series models like the Autoregressive Integrated Moving Average (ARIMA) and Seasonal Autoregressive Integrated Moving Average (SARIMA).\n",
    "\n",
    "Here's an explanation of the Moving Average (MA) model and how it differs from other time series models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af839c-8556-4168-abb8-46313bb6759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Moving Average (MA) Model:\n",
    "\n",
    "In an MA model, the value of a time series at a particular time point is expressed as a linear combination of past white noise or random error terms and possibly some present white noise terms.\n",
    "The model is defined by the order of the MA model, denoted as \"q,\" which indicates the number of lagged error terms used in the model.\n",
    "An MA(q) model is mathematically represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c8292-b961-4dcd-81ae-b92ac093d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X(t) = c + ε(t) + θ(1) * ε(t-1) + θ(2) * ε(t-2) + ... + θ(q) * ε(t-q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247ea24-7511-4e34-ac0b-6ba47ca2ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X(t) is the value of the time series at time t.\n",
    "ε(t), ε(t-1), ..., ε(t-q) are white noise error terms.\n",
    "c is a constant term.\n",
    "θ(1), θ(2), ..., θ(q) are the model's parameters representing the weights of the error terms.\n",
    "The MA model does not involve the time series values themselves; it relies solely on the error terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda61ad-5b95-4535-ad8f-92f0d8040a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Differences from Other Time Series Models:\n",
    "\n",
    "Autoregressive (AR) Model: In contrast to the MA model, the AR model represents a time series as a linear combination of its own past values, rather than past error terms. The AR model captures the autocorrelation in the data by relating each value to its previous values.\n",
    "\n",
    "ARIMA and SARIMA Models: The ARIMA (Autoregressive Integrated Moving Average) and SARIMA (Seasonal ARIMA) models combine both autoregressive and moving average components, along with differencing to handle trends and seasonality. ARIMA includes an autoregressive component, an integrated (differencing) component, and a moving average component. SARIMA extends ARIMA to account for seasonality in addition to the ARIMA components.\n",
    "\n",
    "Exponential Smoothing: Exponential smoothing models, such as the Simple Exponential Smoothing (SES) and Holt-Winters methods, are used for forecasting time series data and involve weighted averages of past observations. Unlike the MA model, exponential smoothing models also incorporate smoothing factors to adapt to changing trends and seasonality.\n",
    "\n",
    "State Space Models: State space models are a more general class of time series models that can include components like AR, MA, seasonal components, and more. They are highly flexible and can handle various time series patterns and data structures.\n",
    "\n",
    "GARCH Models: Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models are used to model the conditional volatility or variance of financial time series. They are different from MA models in that they focus on modeling volatility rather than the time series values themselves.\n",
    "\n",
    "In summary, the Moving Average (MA) model is a specific type of time series model that captures dependencies in a time series by considering weighted averages of past error terms. It is particularly useful for modeling the impact of past shocks or innovations on the current value of the time series. However, for more complex time series patterns that involve autocorrelation, trends, and seasonality, other models like ARIMA, SARIMA, and state space models may be more appropriate.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25a71c-b66a-4e94-a4ab-741f7ba19273",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What is a mixed ARMA model and how does it differ from an AR or MA model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258aadfd-71fa-44e4-bf12-4baeb3404347",
   "metadata": {},
   "outputs": [],
   "source": [
    "A mixed Autoregressive Moving Average (ARMA) model is a time series model that combines both autoregressive (AR) and moving average (MA) components to capture dependencies in time series data. These models are often denoted as ARMA(p, q) models, where \"p\" represents the order of the autoregressive component, and \"q\" represents the order of the moving average component. Mixed ARMA models are more flexible than pure AR or MA models because they can capture both short-term and long-term dependencies in the data.\n",
    "\n",
    "Here's how a mixed ARMA model differs from pure AR or MA models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7972c-f53c-43a2-834c-652d609a739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARMA Model (Mixed ARMA Model):\n",
    "\n",
    "Combines Autoregressive (AR) and Moving Average (MA) components in a single model.\n",
    "Represents a time series as a linear combination of its past values (AR component) and past white noise error terms (MA component).\n",
    "Provides flexibility to capture both short-term (AR) and long-term (MA) dependencies in the data.\n",
    "Mathematically, an ARMA(p, q) model is expressed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335a15d-5236-4ffe-9b4a-b2ae45ff5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X(t) = c + φ(1) * X(t-1) + φ(2) * X(t-2) + ... + φ(p) * X(t-p) + ε(t) + θ(1) * ε(t-1) + θ(2) * ε(t-2) + ... + θ(q) * ε(t-q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
